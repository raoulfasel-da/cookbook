{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Implement PDF ingestion RAG pipeline with DSPy and UbiOps\n",
    "\n",
    "\n",
    "Note: This notebook runs on Python 3.12 and uses UbiOps 4.7.0\n",
    "\n",
    "This notebook shows you how you can implement a PDF ingestion RAG pipeline for your LLM using the pipeline\n",
    "functionality of UbiOps, DSPy, Langchain, PyPDF and Chroma. RAG is a framework that retrieves relevant or supporting context, and\n",
    "adds them to the input. The original query and additional context are then fed to the LLM which produces the final output.\n",
    "To create the proper prompt, we will be using [DSPy](https://dspy.ai/), a framework for programming - rather than prompting - language models.\n",
    "For this tutorial, we will furthermore be using [PyPDF](https://pypdf.readthedocs.io/en/stable/) for the document ingestion and [Chroma](https://www.trychroma.com/) to store the embeddings of the PDF contents. With the pipeline we will build, you can upload a PDF and ask an LLM questions about the contents.\n",
    "We will not deploy the LLM model in this tutorial; instead, we will use a mock-up. This approach eliminates the dependency on GPU usage, enabling you to run the tutorial entirely on CPU deployments.\n",
    "\n",
    "Do furthermore note that it is preferable to run an independent vector store instead of running Chroma inside a deployment. UbiOps deployments are inherently stateless, making it difficult to maintain a persistent vector store. Just for this tutorial, we will use Chroma inside the deployment for simplicity, but be aware that this is not a recommended practice for production environments.\n",
    "\n",
    "The framework will be set-up in a pipeline that contains three deployments: one that ingests and searches through a provided PDF file, another that creates a RAG enhanced prompt with the found information,\n",
    "and one deployment with a mock-up LLM model. This split will allow for easy iteration and testing of the different components.\n",
    "\n",
    "This framework can be set up in your UbiOps environment in five steps:\n",
    "1) Establish a connection with your UbiOps environment\n",
    "2) Create the deployment for the RAG from PDF\n",
    "3) Create the deployment that creates the complete prompt for the LLM\n",
    "4) Create the (mock) deployment for the LLM\n",
    "5) Create a pipeline that combines the three deployments created in step 2,3 and 4\n",
    "\n",
    "For this tutorial the [environments](https://ubiops.com/docs/environments/) will be created implicitly. This means that we\n",
    "will create in total three deployment packages. which could contain these three files:\n",
    "- `deployment.py`, the code that runs when a request is made (i.e., the embedding model & LLM model). This file is mandatory.\n",
    "- `requirements.txt` and a `ubiops.yaml`, which will contain additional dependencies that UbiOps will add to the base environment. These files are optional.\n",
    "\n",
    "\n"
   ],
   "id": "80a327fab08d049b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1) Connecting with the UbiOps API client\n",
    "\n",
    "To use the UbiOps API from our notebook, we need to install the UbiOps Python client library."
   ],
   "id": "95d138a76e0c8d24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install --upgrade ubiops",
   "id": "9a6e554ba49f0959",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To set up a connection with the UbiOps platform API we need the name of your UbiOps project and an API token with `project-editor` permissions.\n",
    "\n",
    "Once you have your project name and API token, paste them below in the following cell before running."
   ],
   "id": "b9824c5edd3964bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import ubiops\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"Token ...\" # Make sure this is in the format \"Token token-code\"\n",
    "PROJECT_NAME = \"...\"  # Fill in your project name here\n",
    "\n",
    "configuration = ubiops.Configuration()\n",
    "configuration.api_key[\"Authorization\"] = API_TOKEN\n",
    "\n",
    "api_client = ubiops.ApiClient(configuration)\n",
    "api = ubiops.api.CoreApi(api_client)"
   ],
   "id": "39b8e124ca6771ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create the deployments for the pipeline",
   "id": "da2cc91deb60ec3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that we have established a connection with our UbiOps environment, we can start creating our deployment packages. Each\n",
    "package could consist of three files (if necessary):\n",
    "- The `deployment.py`, which is where we will define the actual Python code\n",
    "- The `requirements.txt` and a `ubiops.yaml`, which will contain additional dependencies that our codes needs to run properly\n",
    "\n",
    "These deployment packages will be zipped, and uploaded to UbiOps, after which we will add them to a pipeline. The pipeline\n",
    "will consist out of three deployments:\n",
    "- One deployment will be able to ingest a PDF and retrieve information from it\n",
    "- One deployment that will create the prompt for the LLM\n",
    "- One will host the (mock-up) LLM"
   ],
   "id": "6a2618a6ef189b96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "EMBEDDING_DEPLOYMENT_NAME = \"rag-context-from-pdf\"\n",
    "LLM_DEPLOYMENT_NAME = \"mock-up-llm\"\n",
    "PROMPT_DEPLOYMENT_NAME = \"prompt\"\n",
    "\n",
    "deployment_directory = \"deployments\"\n",
    "\n",
    "embedding_deployment_directory_name = \"rag-context-from-pdf-deployment\"\n",
    "llm_deployment_directory_name = \"mock-up-llm-deployment\"\n",
    "prompt_deployment_directory_name = \"prompt-deployment\"\n",
    "\n",
    "embedding_deployment_directory_path = os.path.join(deployment_directory, embedding_deployment_directory_name)\n",
    "llm_deployment_directory_path = os.path.join(deployment_directory, llm_deployment_directory_name)\n",
    "prompt_deployment_directory_path = os.path.join(deployment_directory, prompt_deployment_directory_name)\n"
   ],
   "id": "22da8822bbdec7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2) Create the RAG from pdf deployment (Embedding)",
   "id": "7058826a3508e1a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The first deployment we will be creating is the one with the embedding model. This deployment will extract relevant information from the provided pdf. The retrieved information will be used as context in the final prompt sent to the LLM.",
   "id": "8be4c5b3e4628abd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "os.makedirs(embedding_deployment_directory_path, exist_ok=True)",
   "id": "bff93f103953735b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First we create the `deployment.py`:",
   "id": "fb3efc3d193e16e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%writefile {embedding_deployment_directory_path}/deployment.py\n",
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "class Deployment:\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"Initializing deployment\")\n",
    "        self.embedding_model = os.getenv(\"embedding_model\",\"sentence-transformers/all-MiniLM-l6-v2\")\n",
    "\n",
    "    def request(self, data):\n",
    "        print(\"Processing request\")\n",
    "        context_file = data[\"context_file\"]\n",
    "        query = data[\"query\"]\n",
    "\n",
    "        loader = PyPDFLoader(context_file)\n",
    "\n",
    "        print(\"Cutting document into chunks\")\n",
    "        docs_before_split = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size = 700,\n",
    "            chunk_overlap  = 50,\n",
    "        )\n",
    "        docs_after_split = text_splitter.split_documents(docs_before_split)\n",
    "\n",
    "        print(\"Loading embedding model from huggingface\")\n",
    "        huggingface_embedding_model = HuggingFaceBgeEmbeddings(\n",
    "            model_name=self.embedding_model,\n",
    "            model_kwargs={'device':'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "\n",
    "        print(\"Creating in-memory vector DB\")\n",
    "        vectorstore = Chroma.from_documents(docs_after_split, huggingface_embedding_model)\n",
    "\n",
    "        print(\"retrieving relevant chunks from provided document\")\n",
    "        # Use similarity searching algorithm and return 3 most relevant documents.\n",
    "        retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "        context = [element.page_content for element in retriever.invoke(query)]\n",
    "        context_string = '\\n'.join(context)\n",
    "        return {\"context_string\": context_string}"
   ],
   "id": "23a5b1ed6e41fae4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then we will create the `requirements.txt` and `ubiops.yaml` to specify the required additional dependencies for the code above to run properly.",
   "id": "912d2a9cc9c758fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%writefile {embedding_deployment_directory_path}/requirements.txt\n",
    "langchain==0.3.9\n",
    "langchain-chroma==0.1.4\n",
    "langchain-community==0.3.9\n",
    "langchain-core==0.3.21\n",
    "langchain-text-splitters==0.3.2\n",
    "pypdf==5.1.0\n",
    "sentence-transformers==3.3.1"
   ],
   "id": "5e8a31a3677de1a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%writefile {embedding_deployment_directory_path}/ubiops.yaml\n",
    "apt:\n",
    "  packages:\n",
    "    - build-essential"
   ],
   "id": "e7283467695781d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Now we create the deployment\n",
    "\n",
    "For the deployment we will specify the in- and output for the model:"
   ],
   "id": "dddf318bf91161e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embed_template = ubiops.DeploymentCreate(\n",
    "    name=EMBEDDING_DEPLOYMENT_NAME,\n",
    "    description=\"A deployment that extracts relevant information from a provided pdf and returns this as a string\",\n",
    "    input_type=\"structured\",\n",
    "    output_type=\"structured\",\n",
    "    input_fields=[\n",
    "        {\"name\": \"query\", \"data_type\": \"string\"},\n",
    "        {\"name\": \"context_file\", \"data_type\": \"file\"}\n",
    "    ],\n",
    "    output_fields=[\n",
    "        {\"name\": \"context_string\", \"data_type\": \"string\"}\n",
    "    ],\n",
    "    labels={\"control\": \"embedding\"},\n",
    ")\n",
    "\n",
    "llm_deployment = api.deployments_create(project_name=PROJECT_NAME, data=embed_template)\n",
    "print(llm_deployment)"
   ],
   "id": "fdaa432cc40514fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### And finally we create the version\n",
    "\n",
    "Each deployment can have multiple versions. The version of a deployment defines the coding environment, instance type (CPU or GPU)\n",
    "& size, and other settings:"
   ],
   "id": "19c5849246da091c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=\"v1\",\n",
    "    environment=\"python3-12\",\n",
    "    instance_type_group_name=\"8192 MB + 2 vCPU\",\n",
    "    maximum_instances=1,\n",
    "    minimum_instances=0,\n",
    "    maximum_idle_time=600,  # = 10 minutes\n",
    "    request_retention_mode=\"full\",  # input/output of requests will be stored\n",
    "    request_retention_time=3600,  # requests will be stored for 1 hour\n",
    ")\n",
    "\n",
    "version = api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME, deployment_name=EMBEDDING_DEPLOYMENT_NAME, data=version_template\n",
    ")\n",
    "print(version)"
   ],
   "id": "e0e006df2953375a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then we zip the `deployment package` and upload it to UbiOps (this process can take around 10 minutes).",
   "id": "ad7730cec6ed1410"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "\n",
    "name = shutil.make_archive(embedding_deployment_directory_path, \"zip\", root_dir=deployment_directory, base_dir=embedding_deployment_directory_name)\n",
    "\n",
    "file_upload_result = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=EMBEDDING_DEPLOYMENT_NAME,\n",
    "    version=\"v1\",\n",
    "    file=name,\n",
    ")\n",
    "\n",
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=EMBEDDING_DEPLOYMENT_NAME,\n",
    "    version=\"v1\",\n",
    "    revision_id=file_upload_result.revision,\n",
    ")"
   ],
   "id": "ab94bd82f341c818",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3) Create the prompt node",
   "id": "6c1f1cae40ea7d84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The second deployment we will be creating is the one that creates the prompt for the LLM. This deployment will create a prompt that includes the original query and the context retrieved from the PDF.\n",
    "The output will conform to the OpenAI format.\n",
    "The output will contain a `body` dictionary and a `headers` dictionary, making it easy to use in the next step to send a request to an LLM accepting the OpenAI format.\n",
    "By making use of the DSPy framework, we can easily create the prompt for the LLM.\n",
    "Let's start by creating the `deployment.py`:"
   ],
   "id": "df96148ac1cb47b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "os.makedirs(prompt_deployment_directory_path, exist_ok=True)",
   "id": "70982f651cbdd2c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%writefile {prompt_deployment_directory_path}/deployment.py\n",
    "import dspy\n",
    "import httpretty\n",
    "import json\n",
    "import logging\n",
    "\n",
    "from litellm import APIError\n",
    "\n",
    "\n",
    "class Deployment:\n",
    "    def __init__(self):\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        url = \"http://mocked-api\"\n",
    "        self.lm = dspy.LM(\n",
    "            model=\"openai/meta-llama/Llama-3.2-3B-Instruct\",\n",
    "            api_base=url,\n",
    "            api_key=\"token ...\"\n",
    "        )\n",
    "        dspy.configure(lm=self.lm)\n",
    "        self.rag_prompt = dspy.Predict(\"context, question -> answer\")\n",
    "        self.intercepted_body = None\n",
    "        self.intercepted_header = None\n",
    "        self.register_uri = f\"{url}/chat/completions\"\n",
    "\n",
    "        httpretty.register_uri(\n",
    "            httpretty.POST,\n",
    "            self.register_uri,\n",
    "            body=self.mock_handler,\n",
    "            content_type=\"application/json\"\n",
    "        )\n",
    "\n",
    "    def mock_handler(self, request, uri, response_headers):\n",
    "        logging.info(f\"Intercepted request: {request.body or 'No body detected.'}\")\n",
    "        try:\n",
    "            self.intercepted_body = json.loads(request.body.decode(\"utf-8\")) if request.body else {}\n",
    "            self.intercepted_header = dict(request.headers) if request.headers else {}\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing intercepted request: {e}\")\n",
    "        return [200, response_headers, json.dumps({})]\n",
    "\n",
    "    def request(self, data):\n",
    "        with self.httpretty_context():\n",
    "            try:\n",
    "                logging.info(\"Triggering self.rag_prompt...\")\n",
    "                self.rag_prompt(context=data[\"context\"], question=data[\"question\"])\n",
    "            except APIError:\n",
    "                logging.warning(\"Request failed or intercepted.\")\n",
    "\n",
    "            assert self.intercepted_body is not None, \"No body intercepted!\"\n",
    "            assert self.intercepted_header is not None, \"No headers intercepted!\"\n",
    "\n",
    "            return {\n",
    "                \"body\": self.intercepted_body,\n",
    "                \"headers\": self.intercepted_header,\n",
    "            }\n",
    "\n",
    "    def httpretty_context(self):\n",
    "        class HTTPrettyContext:\n",
    "            def __enter__(self):\n",
    "                httpretty.enable()\n",
    "\n",
    "            def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "                httpretty.disable()\n",
    "                httpretty.reset()\n",
    "\n",
    "        return HTTPrettyContext()\n"
   ],
   "id": "5c7f5866b802d484",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's create the `requirements.txt` to install the required Python packages.\n",
    "We will not need a `ubiops.yaml` file for this deployment, as no extra OS-level dependencies are required."
   ],
   "id": "8edcef802d720632"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%writefile {prompt_deployment_directory_path}/requirements.txt\n",
    "dspy\n",
    "httpretty"
   ],
   "id": "7eded748814c7d8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Now we create the deployment\n",
    "\n",
    "For the deployment we will specify the in- and output for the model:"
   ],
   "id": "bd12682719f8f8de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prompt_template = ubiops.DeploymentCreate(\n",
    "    name=PROMPT_DEPLOYMENT_NAME,\n",
    "    description=\"A deployment that creates a prompt for the LLM model\",\n",
    "    input_type=\"structured\",\n",
    "    output_type=\"structured\",\n",
    "    input_fields=[\n",
    "        {\"name\": \"question\", \"data_type\": \"string\"},\n",
    "        {\"name\": \"context\", \"data_type\": \"string\"}\n",
    "    ],\n",
    "    output_fields=[\n",
    "        {\"name\": \"body\", \"data_type\": \"dict\"},\n",
    "        {\"name\": \"headers\", \"data_type\": \"dict\"}\n",
    "    ],\n",
    "    labels={\"control\": \"prompt\"},\n",
    ")\n",
    "\n",
    "llm_deployment = api.deployments_create(project_name=PROJECT_NAME, data=prompt_template)\n",
    "print(llm_deployment)"
   ],
   "id": "5eca74e5dc1746a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### And finally we create the version\n",
    "\n",
    "Each deployment can have multiple versions. The version of a deployment defines the coding environment, instance type (CPU or GPU)\n",
    "& size, and other settings:"
   ],
   "id": "d016d37fcc4719f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=\"v1\",\n",
    "    environment=\"python3-12\",\n",
    "    instance_type_group_name=\"4096 MB + 1 vCPU\",\n",
    "    maximum_instances=1,\n",
    "    minimum_instances=0,\n",
    "    maximum_idle_time=600,  # = 10 minutes\n",
    "    request_retention_mode=\"full\",  # input/output of requests will be stored\n",
    "    request_retention_time=3600,  # requests will be stored for 1 hour\n",
    ")\n",
    "\n",
    "version = api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME, deployment_name=PROMPT_DEPLOYMENT_NAME, data=version_template\n",
    ")\n",
    "print(version)"
   ],
   "id": "9da489dab4b3d0de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then we zip the `deployment package` and upload it to UbiOps (this process can take between 5-10 minutes).",
   "id": "9074dde3ae6a46f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "\n",
    "name = shutil.make_archive(prompt_deployment_directory_path, \"zip\", root_dir=deployment_directory, base_dir=prompt_deployment_directory_name)\n",
    "\n",
    "file_upload_result = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=PROMPT_DEPLOYMENT_NAME,\n",
    "    version=\"v1\",\n",
    "    file=name,\n",
    ")\n",
    "\n",
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=PROMPT_DEPLOYMENT_NAME,\n",
    "    version=\"v1\",\n",
    "    revision_id=file_upload_result.revision,\n",
    ")"
   ],
   "id": "332912e350f18fc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4) Create the LLM inference node",
   "id": "9daa0c8ab653edf8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "At last, we will create a mock-up LLM deployment. This deployment will mock the LLM model, and return a fixed response.\n",
    "Since the output of our previous deployment is conforming to the OpenAI format, it will be easy to change this deployment to use an actual LLM model.\n",
    "Let's start by creating the `deployment.py`:"
   ],
   "id": "e3caa0e443c3f696"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "os.makedirs(llm_deployment_directory_path, exist_ok=True)",
   "id": "3c8a7271d11706dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%writefile {llm_deployment_directory_path}/deployment.py\n",
    "\n",
    "class Deployment:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def request(self, data):\n",
    "        # Extract \"body\" and \"headers\" from the dictionary\n",
    "        body = data.get(\"body\", {})\n",
    "        headers = data.get(\"headers\", {})\n",
    "\n",
    "        # Extract and format body content\n",
    "        max_tokens = body.get(\"max_tokens\", \"N/A\")\n",
    "        model = body.get(\"model\", \"N/A\")\n",
    "        temperature = body.get(\"temperature\", \"N/A\")\n",
    "\n",
    "        messages = body.get(\"messages\", [])\n",
    "        formatted_messages = \"\\n\".join(\n",
    "            f\"- Role: {message.get('role', 'N/A')}\\n  Content:\\n  {message.get('content', 'N/A')}\"\n",
    "            for message in messages\n",
    "        )\n",
    "\n",
    "        # Extract and format headers content\n",
    "        formatted_headers = \"\\n\".join(f\"{key}: {value}\" for key, value in headers.items())\n",
    "\n",
    "        # Combine all parts into a formatted string\n",
    "        formatted_output = (\n",
    "            \"=== Body ===\\n\"\n",
    "            f\"Model: {model}\\n\"\n",
    "            f\"Max Tokens: {max_tokens}\\n\"\n",
    "            f\"Temperature: {temperature}\\n\"\n",
    "            f\"Messages:\\n{formatted_messages}\\n\\n\"\n",
    "            \"=== Headers ===\\n\"\n",
    "            f\"{formatted_headers}\"\n",
    "        )\n",
    "\n",
    "        return {\"output\": formatted_output}"
   ],
   "id": "c0e8c08a3d42b2fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Now we create the deployment\n",
    "\n",
    "For the deployment we will specify the in- and output for the model:"
   ],
   "id": "83d53c8efddf0cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "llm_mock_template = ubiops.DeploymentCreate(\n",
    "    name=LLM_DEPLOYMENT_NAME,\n",
    "    description=\"A deployment that creates a mock LLM output\",\n",
    "    input_type=\"structured\",\n",
    "    output_type=\"structured\",\n",
    "    input_fields=[\n",
    "        {\"name\": \"body\", \"data_type\": \"dict\"},\n",
    "        {\"name\": \"headers\", \"data_type\": \"dict\"}\n",
    "    ],\n",
    "    output_fields=[\n",
    "        {\"name\": \"output\", \"data_type\": \"string\"}\n",
    "    ],\n",
    "    labels={\"control\": \"llm_mock\"},\n",
    ")\n",
    "\n",
    "llm_deployment = api.deployments_create(project_name=PROJECT_NAME, data=llm_mock_template)\n",
    "print(llm_deployment)"
   ],
   "id": "dc26129ac84440a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### And finally we create the version\n",
    "\n",
    "Each deployment can have multiple versions. The version of a deployment defines the coding environment, instance type (CPU or GPU)\n",
    "& size, and other settings:"
   ],
   "id": "fb2ec39496e82c98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=\"v1\",\n",
    "    environment=\"python3-12\",\n",
    "    instance_type_group_name=\"512 MB + 0.125 vCPU\",\n",
    "    maximum_instances=1,\n",
    "    minimum_instances=0,\n",
    "    maximum_idle_time=600,  # = 10 minutes\n",
    "    request_retention_mode=\"full\",  # input/output of requests will be stored\n",
    "    request_retention_time=3600,  # requests will be stored for 1 hour\n",
    ")\n",
    "\n",
    "version = api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME, deployment_name=LLM_DEPLOYMENT_NAME, data=version_template\n",
    ")\n",
    "print(version)"
   ],
   "id": "5dad204a179157f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then we zip the `deployment package` and upload it to UbiOps (this process can take between 5-10 minutes).",
   "id": "b94c462675469453"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "\n",
    "name = shutil.make_archive(llm_deployment_directory_path, \"zip\", root_dir=deployment_directory, base_dir=llm_deployment_directory_name)\n",
    "\n",
    "file_upload_result = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=LLM_DEPLOYMENT_NAME,\n",
    "    version=\"v1\",\n",
    "    file=name,\n",
    ")\n",
    "\n",
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=LLM_DEPLOYMENT_NAME,\n",
    "    version=\"v1\",\n",
    "    revision_id=file_upload_result.revision,\n",
    ")"
   ],
   "id": "40bd57fb422b598c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "All of our deployments are now ready and uploaded to UbiOps. We can now create a pipeline that combines the three deployments.",
   "id": "253070483e5020f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5) Create the pipeline",
   "id": "6f0b6b0236415a02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we create a pipeline that orchastrates the workflow between the deployments above. When a request will be made to this pipeline\n",
    "the first deployment will ingest the provided PDF and will search for information that's relevant to the user's query. This information will then be sent to the prompt deployment which will create a prompt out of the query and context.\n",
    " This prompt will then be sent to the LLM (mock) deployment, which will answer the user's query based on the provided information from the PDF.\n",
    " The pipeline will be as follows:\n",
    "\n"
   ],
   "id": "9eee281e4369e02f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![pipeline-image](https://storage.googleapis.com/ubiops/tutorial-helper-files/dspy-pipeline-tutorial/DSPY-tutorial-pipeline-image.png)",
   "id": "3ceb15d304537d3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For a pipeline you will have to define an input & output and create a version, as with a deployment. In addition to this we\n",
    "will also need to define the objects (i.e, deployments) and how to orchestrate the workflow (i.e., how to attach each object\n",
    "to each other)."
   ],
   "id": "9beb585dd403b889"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First we create the pipeline:",
   "id": "93f1ca0860235e2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "PIPELINE_NAME = \"llama-with-rag\"\n",
    "PIPELINE_VERSION = \"v1\""
   ],
   "id": "91888116022fc463",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pipeline_template = ubiops.PipelineCreate(\n",
    "    name=PIPELINE_NAME,\n",
    "    description=\"A pipeline to prepare prompts, and generate text using llama 3.3\",\n",
    "    input_type=\"structured\",\n",
    "    input_fields=[\n",
    "        {\"name\": \"query\", \"data_type\": \"string\"},\n",
    "        {\"name\": \"context_file\", \"data_type\": \"file\"}\n",
    "    ],\n",
    "    output_type=\"structured\",\n",
    "    output_fields=[\n",
    "        {\"name\": \"output\", \"data_type\": \"string\"}\n",
    "    ],\n",
    "    labels={\"demo\": \"llama-3-3-RAG\"},\n",
    ")\n",
    "\n",
    "api.pipelines_create(project_name=PROJECT_NAME, data=pipeline_template)"
   ],
   "id": "12b4cd76cd3e089e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then we define the objects, and how to attach the objects together:",
   "id": "406e25cf30612c6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "objects = [\n",
    "    # RAG\n",
    "    {\n",
    "        \"name\": EMBEDDING_DEPLOYMENT_NAME,\n",
    "        \"reference_name\": EMBEDDING_DEPLOYMENT_NAME,\n",
    "        \"version\": \"v1\",\n",
    "    },\n",
    "    # PROMPT\n",
    "    {\n",
    "        \"name\": PROMPT_DEPLOYMENT_NAME,\n",
    "        \"reference_name\": PROMPT_DEPLOYMENT_NAME,\n",
    "        \"version\": \"v1\"\n",
    "    },\n",
    "    # LLM-model\n",
    "    {\n",
    "        \"name\": LLM_DEPLOYMENT_NAME,\n",
    "        \"reference_name\": LLM_DEPLOYMENT_NAME,\n",
    "        \"version\": \"v1\"\n",
    "    }\n",
    "]\n",
    "\n",
    "attachments = [\n",
    "    # start --> RAG\n",
    "    {\n",
    "        \"destination_name\": EMBEDDING_DEPLOYMENT_NAME,\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"source_name\": \"pipeline_start\",\n",
    "                \"mapping\": [\n",
    "                    {\n",
    "                        \"source_field_name\": \"query\",\n",
    "                        \"destination_field_name\": \"query\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"source_field_name\": \"context_file\",\n",
    "                        \"destination_field_name\": \"context_file\",\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    # RAG + pipeline-start -> PROMPT\n",
    "    {\n",
    "        \"destination_name\": PROMPT_DEPLOYMENT_NAME,\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"source_name\": EMBEDDING_DEPLOYMENT_NAME,\n",
    "                \"mapping\": [\n",
    "                    {\n",
    "                        \"source_field_name\": \"context_string\",\n",
    "                        \"destination_field_name\": \"context\",\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"source_name\": \"pipeline_start\",\n",
    "                \"mapping\": [\n",
    "                    {\n",
    "                        \"source_field_name\": \"query\",\n",
    "                        \"destination_field_name\": \"question\",\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    # PROMPT -> LLM\n",
    "    {\n",
    "        \"destination_name\": LLM_DEPLOYMENT_NAME,\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"source_name\": PROMPT_DEPLOYMENT_NAME,\n",
    "                \"mapping\": [\n",
    "                    {\n",
    "                        \"source_field_name\": \"body\",\n",
    "                        \"destination_field_name\": \"body\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"source_field_name\": \"headers\",\n",
    "                        \"destination_field_name\": \"headers\",\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    # LLm -> pipeline end\n",
    "    {\n",
    "        \"destination_name\": \"pipeline_end\",\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"source_name\": LLM_DEPLOYMENT_NAME,\n",
    "                \"mapping\": [\n",
    "                    {\n",
    "                        \"source_field_name\": \"output\",\n",
    "                        \"destination_field_name\": \"output\",\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]"
   ],
   "id": "89801f86e59b243f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pipeline_template = ubiops.PipelineVersionCreate(\n",
    "    version=PIPELINE_VERSION,\n",
    "    request_retention_mode=\"full\",\n",
    "    objects=objects,\n",
    "    attachments=attachments,\n",
    ")\n",
    "\n",
    "api.pipeline_versions_create(\n",
    "    project_name=PROJECT_NAME, pipeline_name=PIPELINE_NAME, data=pipeline_template\n",
    ")"
   ],
   "id": "93248b7044ae544",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## And there you have it!\n",
    "\n",
    "That is all you need to know about how to set-up a RAG from file framework in UbiOps, using Langchain, Chroma and PyPDF.\n",
    "\n",
    "Let's test it out by providing our pipeline with Nike's annual public SEC report and asking about its contents!"
   ],
   "id": "ab2acb1603a9ecf5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ubiops.utils import upload_file\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "url = \"https://s1.q4cdn.com/806093406/files/doc_downloads/2023/414759-1-_5_Nike-NPS-Combo_Form-10-K_WR.pdf\"\n",
    "location, _ = urlretrieve(url, \"nike_sec.pdf\")\n",
    "\n",
    "file_uri= upload_file(\n",
    "    client=api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    file_path=location\n",
    ")\n",
    "api.pipeline_requests_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    pipeline_name=PIPELINE_NAME,\n",
    "    data={\n",
    "        \"query\": \"What was Nike's revenue in 2023?\",\n",
    "        \"context_file\": file_uri\n",
    "    },\n",
    ")"
   ],
   "id": "27c9cfd2f2c6fa93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As you can see, the pipeline is able to retrieve relevant information from the PDF, create a prompt for the LLM, and return a (mock) answer to the user's query.",
   "id": "761f0287d8ed6859"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
