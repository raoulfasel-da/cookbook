{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvWK1vVgU1kZ"
   },
   "source": [
    "# Deploy Gemma 2B with streaming on UbiOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iD3EPDKFU1kf"
   },
   "source": [
    "This tutorial will help you create a cloud-based inference API endpoint for the gemma-2-2b-it model, using UbiOps. The generated text can be streamed back to an end-user. Gemma-2-2b-it is a lightweight LLM developed by Google, that can run on a CPU-type instance (does not require a GPU). It was developed by Google, and is available through Huggingface.\n",
    "\n",
    "Note that Gemma is a gated model, so you will need to have a valid [Huggingface token](https://huggingface.co/docs/hub/en/security-tokens) with sufficient permissions if you want to download the gemma-2-2b-it from Huggingface. You can apply for one in [the repository of the respective model](https://huggingface.co/google/gemma-2-2b-it). The model can also be uploaded to your UbiOps bucket, and downloaded from there.\n",
    "\n",
    "In this tutorial we will walk you through.\n",
    "\n",
    "1. Connecting with the UbiOps API client\n",
    "2. Creating a deployment for the Gemma 2 2B model\n",
    "3. Create a request and stream the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmHv_MmkU1ki"
   },
   "source": [
    "## 1. Connecting with the UbiOps API client\n",
    "To use the UbiOps API from our notebook, we need to install the UbiOps Python client library first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU ubiops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up a connection with the UbiOps platform API we need the name of your UbiOps project and an API token with `project-editor` permissions. See our documentation on how to [create a token](https://ubiops.com/docs/organizations/service-users/#service-users-and-api-tokens).\n",
    "\n",
    "Once you have your project name and API token, paste them below in the following cell before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ug_zkj78U1ki",
    "outputId": "6b0a490e-3b73-43f4-aa3b-2705cf850c92"
   },
   "outputs": [],
   "source": [
    "import ubiops\n",
    "from datetime import datetime\n",
    "\n",
    "DEPLOYMENT_NAME = f\"gemma-2-{datetime.now().date()}\"\n",
    "DEPLOYMENT_VERSION = \"v1\"\n",
    "\n",
    "# Define our tokens\n",
    "API_TOKEN = \"<API TOKEN>\"  # Make sure this is in the format \"Token token-code\"\n",
    "PROJECT_NAME = \"<PROJECT_NAME>\"  # Fill in your project name here\n",
    "HF_TOKEN = \"<HF_TOKEN>\"  # Format: \"hf_xyz\"\n",
    "\n",
    "\n",
    "# Initialize client library\n",
    "configuration = ubiops.Configuration(host=\"https://api.ubiops.com/v2.1\")\n",
    "configuration.api_key[\"Authorization\"] = API_TOKEN\n",
    "\n",
    "# Establish a connection\n",
    "client = ubiops.ApiClient(configuration)\n",
    "api = ubiops.CoreApi(client)\n",
    "print(api.service_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8SxE3ttU1kl"
   },
   "source": [
    "## 2. Creating a deployment for Gemma 2 2B\n",
    "\n",
    "We will now set up our deployment that runs the Gemma-2-2b-it model with streaming capabilities. First we create our deployment \n",
    "package - a directory in which our deployment files are added.\n",
    "\n",
    "The deployment code is added to a `deployment.py` file, which has a `Deployment` class and two methods:\n",
    "- `__init__` will run when the deployment starts up and can be used to load models, data, artifacts and other requirements \n",
    "for inference.\n",
    "- `request()` will run every time a call is made to the model REST API endpoint and includes all the logic for processing \n",
    "data.\n",
    "\n",
    "Additionally, we will add the dependencies that our code requires to a deployment package  a `requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_package_dir = \"deployment_package\"\n",
    "\n",
    "!mkdir {deployment_package_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add the `deployment.py` to the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krAgLhlhU1km"
   },
   "outputs": [],
   "source": [
    "%%writefile {deployment_package_dir}/deployment.py\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "class Deployment:\n",
    "\n",
    "    def __init__(self, base_directory, context):\n",
    "\n",
    "        # Log in to Hugging Face\n",
    "        token = os.environ[\"HF_TOKEN\"]\n",
    "        login(token=token)\n",
    "\n",
    "        # Download Gemma from Hugging Face\n",
    "        model_id = os.environ.get(\"MODEL_ID\", \"google/gemma-2-2b-it\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "        # You can change the system prompt by adding an environment variable to your deployment (version)\n",
    "        self.system_prompt = os.environ.get(\n",
    "            \"SYSTEM_PROMPT\",\n",
    "            \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "        )\n",
    "\n",
    "    def request(self, data, context):\n",
    "\n",
    "        user_prompt = data\n",
    "        streaming_callback = context[\"streaming_update\"]\n",
    "\n",
    "        # Prepare the chat prompt with the system message and user input\n",
    "        chat = [{\"role\": \"user\", \"content\": f\"{self.system_prompt} \\n {user_prompt}\"}]\n",
    "        print(\"Applied chat: \\n\", chat)\n",
    "\n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            chat, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = self.tokenizer(\n",
    "            prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True)\n",
    "\n",
    "        generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=256)\n",
    "        \n",
    "        # The TextIteratorStreamer requires a thread which we start here\n",
    "        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n",
    "        thread.start()\n",
    "\n",
    "        generated_text = \"\"\n",
    "        for new_text in streamer:\n",
    "            # We use the streaming_callback from UbiOps to send partial updates\n",
    "            streaming_callback(new_text)\n",
    "            generated_text += new_text\n",
    "\n",
    "        return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dependency file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {deployment_package_dir}/requirements.txt\n",
    "huggingface-hub\n",
    "transformers==4.45.2\n",
    "torch==2.4.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPKcskNUU1km"
   },
   "source": [
    "### Create the deploymenta & deployment version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAEsUt5cU1kn"
   },
   "source": [
    "#### Create the deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5IC5Lf8U1kn"
   },
   "outputs": [],
   "source": [
    "# Create the deployment\n",
    "deployment_template = ubiops.DeploymentCreate(\n",
    "    name=DEPLOYMENT_NAME,\n",
    "    input_type=\"structured\",\n",
    "    output_type=\"plain\",\n",
    "    input_fields=[{\"name\": \"prompt\", \"data_type\": \"string\"}],\n",
    ")\n",
    "\n",
    "api.deployments_create(project_name=PROJECT_NAME, data=deployment_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let us add our Huggingface token as a secret environment variable to our deployment, so that all of our deployment versions\n",
    "are authenticated to download the relevant model files from Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response = api.deployment_environment_variables_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    data=ubiops.EnvironmentVariableCreate(name=\"HF_TOKEN\", value=HF_TOKEN, secret=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAwFlqQ3U1kn"
   },
   "source": [
    "#### And a deployment version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DQm_pcIU1kn"
   },
   "outputs": [],
   "source": [
    "# Create the version\n",
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    environment=\"python3-12\",\n",
    "    instance_type_group_name=\"12288 MB + 3 vCPU\",\n",
    "    maximum_instances=1,\n",
    "    minimum_instances=0,\n",
    "    maximum_idle_time=600,  # = 10 minutes\n",
    "    request_retention_mode=\"full\",\n",
    ")\n",
    "\n",
    "api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME, deployment_name=DEPLOYMENT_NAME, data=version_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ui06YpKU1ko"
   },
   "outputs": [],
   "source": [
    "# And now we zip our code (deployment package) and push it to the version\n",
    "\n",
    "import shutil\n",
    "\n",
    "deployment_code_archive = shutil.make_archive(\n",
    "    deployment_package_dir, \"zip\", deployment_package_dir\n",
    ")\n",
    "\n",
    "upload_response = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file=deployment_code_archive,\n",
    ")\n",
    "print(upload_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the deployment is finished building. Your first iteration will take around 10 minutes because a new environment is built.\n",
    "Consecutive deployment code iterations will take only a couple of seconds because the environment was already built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    revision_id=upload_response.revision,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fwi2qY7LU1ko"
   },
   "source": [
    "You can check out the new deployment and the building process in the \n",
    "[UbiOps WebApp](https://app.ubiops.com) in the meantime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not happy with the default `SYSTEM_PROMPT` that we provided, you can add your own system prompt here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_SYSTEM_PROMPT = \"You are a friendly chatbot who always responds in the style of a man with a mission\"\n",
    "\n",
    "api_response = api.deployment_version_environment_variables_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    data=ubiops.EnvironmentVariableCreate(\n",
    "        name=\"SYSTEM_PROMPT\", value=CUSTOM_SYSTEM_PROMPT, secret=False\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzyr-AHlU1ko"
   },
   "source": [
    "## 3. Create a request and stream the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jc07uah6U1ko"
   },
   "source": [
    "We can now send our first prompt to our Gemma LLM! On the first spin-up, the model will need to be downloaded from Huggingface,\n",
    "resulting in a cold-start time of a couple of minutes for your deployment instance. Subsequent requests should be handled faster. \n",
    "You can check the UbiOps User Interface to see the status and logs of your deployment instance while it is spinning up.\n",
    "Once your instance is ready, tokens are streamed as they are generated by the Gemma model. Do note that this model has a \n",
    "rather long inference time in general. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"prompt\": \"I accidentally brought the Black Plague on my ship. How do I blame the crew?\"\n",
    "}\n",
    "\n",
    "# Create a streaming deployment request\n",
    "\n",
    "for item in ubiops.utils.stream_deployment_request(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    data=data,\n",
    "    timeout=3600,\n",
    "    full_response=False,\n",
    "    ):\n",
    "    print(item, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's it! You now have your own on-demand, scalable Gemma 2 2B Instruct model running in the cloud, with a REST API that you can reach from anywhere!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
